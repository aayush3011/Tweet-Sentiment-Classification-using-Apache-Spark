<html>
<head>
<title>
<h1> CS 491 - Twitter Sentiment Classification using Apache Spark</h1>
</title>
</head>
<body>
<h1>CS 491 - Twitter Sentiment Classification using Apache Spark</h1>
<ol>
	<li>Describe your tweet processing steps.</li>
	<p>So, for data cleaning of my tweets I used the following techniques:</p>
		<ol>
			<li>I used the HTML parser to filter the data of html type which often comes with the tweets.</li>
			<li>Secondly, I removed the appostophes that were present in the tweets. Say for example if there is a word "I'm" then my regex would change the word to "I am" and hence the appostophes are removed.</li>
			<li>Then, I removed the stopwords from the tweets using the stopwords file provided in the last homework. </li>
			<li>Then, I checked if the first element of every word in the tweet is an alphabet.</li>
			<li>Then, I removed the usernames for the tweets using a regex.</li>
			<li>Then, I removed the url's for the tweets using a regex.</li>
			<li>Then, I striped the hashes from the tweet as "#xyz" to "xyz".</li>
			<li>Then, I removed the punctutaion from the tweets.</li>
			<li>Then, I used the porter stemmer to stem the words to their generic form.</li>
			<li>And, at last I removed the repeated letters in a word as "exciteddddd" to "excitedd".</li>
		</ol>
	<li>Describe your feature space. Did you decide to use unigrams, bigrams, or both? What is the size of your feature space?</l>
		<p>We used a Bag of Words of Model as our feature spacce. The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.<br>
			I used unigrams as the word sin the model.<br>
			The size of the feature space is the default size which will come up when we will claen all the data.</p>
	<li>Describe any extra work (i.e. parameter tuning) you did on three classifiers: NB, LOG and Decision Tree(DT). How did it help?</li>
		<p>So, I cleaned the data by removing the appostophes which tunes the bag of words model and helps in achieving a better accuracy. I have also removed all the url's and usernames as they don't play any part in the happiness or unhappiness of a tweet</p>
	<li>For three classifiers (NB, LOG and DT), report training accuracy, 10-fold cross-validation accuracy, test accuracy, avg precision/recall/f1-score on test, and the confusion matrix on test. Any findings?</li>
		<ol>
				<li>Naive Bayes</li>
					<ul>
						<li>10-fold cross-validation accuracy: 71.34%</li>
						<li>Test accuracy: 74.37%</li>
						<li>Precision: .7437</li>
						<li>Recall: .7437</li>
						<li>f1-score: .7437</li>
						<li>Confusion Matrix: [ 138.,   39.],[53.,  129.]</li>
					</ul>
				<li>Logistic Regression</li>
					<ul>
						<li>10-fold cross-validation accuracy: 72.28%</li>
						<li>Test accuracy: 77.4%</li>
						<li>Precision: .7734</li>
						<li>Recall:.7734 </li>
						<li>f1-score: .7734</li>
						<li>Confusion Matrix: [134.,   43.],[  38.,  144.]</li>
					</ul>
		</ol>
	<li>For three classifiers (NB, LOG and DT), plot training accuracy, 10-fold cross-validation accuracy and test accuracy together using matplotlib. You can check out this tutorial. Which classifier overfits the most?</li>	
		<p><img src="accuracies.png"></p>
	<li>Describe the following terms in the context of the assignment: precision, recall, f1-score, confusion matrix (true positive, true negative, false positive, false negative).</li>
		<ul>
			<li> Precision: In information retrieval contexts, precision and recall are defined in terms of a set of retrieved documents and a set of relevant documents.</li>
			<li>Recall: Recall in information retrieval is the fraction of the documents that are relevant to the query that are successfully retrieved.</li>
			<li> f1-score: In statistical analysis of binary classification, the F1 score (also F-score or F-measure) is a measure of a test's accuracy. It considers both the precision p and the recall r of the test to compute the score: p is the number of correct positive results divided by the number of all positive results, and r is the number of correct positive results divided by the number of positive results that should have been returned. The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst at 0.</li>
			<li> Confusion Matrix: In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix.Each column of the matrix represents the instances in a predicted class while each row represents the instances in an actual class (or vice-versa).The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabeling one as another).</li>
		</ul>
	<li>For NB and LOG, plot the ROC curve and report the area under the curve. What do you learn from the ROC curve?</li>
		<p><img src="roc_curve-on testing for Logistic regression.png"></p>
	<li>Report top 20 most informative features for all three classifiers. Any findings?</li>
		<p>There is no funtion or method to find the top 20 most informative features for all three classifiers.</p>
	<li>Which classifier performs the best? Why?</li>
	 <p>I think that Logistic regression perfomrs much better than Naive Bayes as seen in the accuacries found by me. Moreover Naive Bayes takes many assumptaitons so in any case logistic regression will perfomr mauch better than it.</p>
	<li>Using the best classifer, print some test tweets that are classified correctly or incorreclty along with their prediction probabilities. Among correctly classified tweets, print 5 tweets with highest predicted probailities. Repeat this for incorrectly classified tweets.</li>
		<ol>	
			<li>5 tweets with highest predicted probabilities:
			 	<ol>
			 		<li>so many insecurities and stress - just because im moving to a new room : .821</li>
			 		<li>Obama Win?: .903</li>
			 		<li>Barack Obama shows his funny side : .121</li>
			 		<li>Talk is Cheap: Bing that, I?ll stick with Google. http://bit.ly/XC3C8: .7986</li>
			 		<li>I hate Time Warner! Soooo wish I had Vios. Cant watch the fricken Mets game w/o buffering. I feel like im watching free internet porn: .164</li>
			 	</ol>
			</li>
			<li>5 tweets with lowest predicted probabilities
				<ol>
			 		<li>is being fucked by time warner cable. didnt know modems could explode. and Susan Boyle sucks too! : .13</li>
			 		<li>@mikefish  Fair enough. But i have the Kindle2 and I think it's perfect  :): .92</li>
			 		<li>Excited about seeing Bobby Flay and Guy Fieri tomorrow at the Great American Food &amp; Music Fest! : .45</li>
			 		<li>GOT MY WAVE SANDBOX INVITE! Extra excited! Too bad I have class now... but I'll play with it soon enough! #io2009 #wave: .86</li>
			 		<li>Gonna go see Bobby Flay 2moro at Shoreline. Eat and drink. Gonna be good.: .42</li>
			 	</ol>
			</li>
		</ol>
</ol>


</body>
</html>